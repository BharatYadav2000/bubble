################## S3 #####################################

 S3(Simple storage service) is advertises as infinity storage , beacuse the complete aws storage is not used by anyone.
 
 S3 console is a global but s3 bucket is regional.
 
 Many website use Amaon S3 as a backbone.
 Many AWS services uses Amazon S3 as an integration as well. 
 
 ### S3 use cases ###
 
 * Backup and storage
 * Disaster Recovery(we can copy/store data in multiple regions).
 * Archive ( we can archive data for cost saving).
 * Hybrid Cloud Storage(means increase the storage of on-premise on s3 cloud).
 * Application hosting , media hosting.
 * Data Lakes & big data analytics (means store large/big data and analyze it directly on amazon s3).
 * Software delivery
 * static website
 
 ### S3 Bucket ###
 
* S3 allow people to store objects(files) in buckets(directories).
* we can create folder inside object.
* S3 bucket must have a globally unique name (across all regions all accounts).
* S3 looks like a global service but buckets are created in a region.
* Naming convention:- No uppercase,No underscore,3-36characters long, Not an ip ,must start with lowercase letter or number.

* Object(files) have a key.
* The key is the FULL path: e.g - s3://my-bucket/myfile.txt
* The key is composed of prefix + object name.

## S3 Bucket Object ##

* object is nothing it is a file.
* you can store unlimited object in s3.
* Maximum one object limit size is 5TB(5000GB).
* If uploading more than 5GB , must use multi-part upload.

#########################################

* By default the bucket & object created is private so it is not accessed through key/link by another user. we can make it public.

################ S3 Security ###############################

(1) User-Based Security :- we can define IAM user and assign policy to allow them to get access of s3 bucket.(this security is applied only when the IAM user of same account want to access s3 , for other users we define bucket policy).

(2) Resource based Security :- 
* Bucket Policy :- In this rule is directly attached into s3 bucket to allow or deny request coming from other accounts.(this security is applied only when the user is out of the account/orgainization want to access s3)

* Object Access Control List(ACL) : -Object ACL allows you to specify fine-grained access permissions at the object level within an S3 bucket. By default, S3 buckets and objects are private, meaning only the AWS account that owns the bucket or object has access to them. However, with Object ACL, you can grant read or write access to other AWS accounts or even make objects publicly accessible.

* Bucket Access Control List(ACL) :- In addition to Object ACLs, Amazon S3 also provides Bucket ACL (Access Control List) to manage access permissions at the bucket level. Bucket ACL allows you to control who can perform operations on the bucket itself, such as listing objects, uploading objects, or modifying the bucket's configuration.

(3) Encryption:- encrypt object in s3 using encryption key.

####
There are only two ways to give access to other account to access s3:-
(1) Using IAM attach s3 policy to user/other account. (this security is applied only when the IAM user of same account want to access s3)
          OR
(2) creating s3 bucket policy.(this security is applied only when the user is out of the account/orgainization want to access s3)

###
Use S3 bucket for policy to:
* Grant public access to bucket.
* force object to be encrypted at upload.
*Grant access to another account(cross account).

########## S3 Bucket Policy ###########################

JSON based policy:-

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "GrantReadAccessToObjects",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ACCOUNT-ID:root"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::YOUR-BUCKET-NAME/*"
    }
  ]
}

Resource : bucket/object link/key
Action: set of API to allow or deny
Effect: Allow/Deny
Principal: The account or user to apply policy
Sid: access type

################# S3 WEBSITE ##################################

We made the object public so we can host the website onto Amazon S3.
S3 is the best way to host the static website worlwide.

################ S3 Versioning ###############################

To replace/update in the existing website file and also keep the previous version of website we use s3 versioning.
It is enabled at the bucket level.
Anything you update in a file , it will get a new version.
It is best practice to version your buckets - * if someone delete a file , it will recover from previous version. * Easy to roll back to previous version.

############### S3 Access Logs ###################################

For audit(inspection) purpose, you may want to log all access to s3 buckets.

Any request made to s3 from any account , authorized or denied will be logged into another s3 bucket.

The data can be analyzed using data analysis tools.

Very helpful to come down to the root cause of an issue, or audit usage , view suspicious pattern etc.

############# S3 Replication(copy) ##################################

You can enable replication in s3 for extra buckets.
Must enable versioning in source and destinition.
Buckets can be in differnt accounts.
Copying is asynchronous(run independently).
Object/files in s3 before enabling replication are not replicated to other s3 bucket , only object/file uploaded after enabling replication are replicated to s3 bucket(means when you enable the replication in s3 it only copy the object/file to another s3 , not copy the previous/already presented files).But it(copying existing files) can be done by using Batch operation service(this service is like rsync, databacup schedule).

There are two types of S3 Replications:-

(1) Cross Region Replication (CRR) :- copy s3 data from one region to another region .
Use-cases of CRR - compliance(govt. protocol), lower latency access, replication across account.
(2) Same Region Replication (SRR):- copy s3 data in same region.
Use-cases of SRR - log aggregation , live replication between production and test account.

The process of replication is same for both(CRR & SRR).

################# S3 Storage Class ####################################

In S3 there are different options to store files in different storage classes and these can give us some cost saving. 

S3 Durability :-
S3 has high durability(99.999999999% )(after dot 9s time 9 and total 11 9s) of object/file across multiple availability zones. This durability is by default same for all s3 storage classes(standard, glacier etc).

S3 Availability :- 
Measure how readily available a service is .
S3 standard has 99.99% availability .
Availability will vary based on storage class.

Types of S3 storage classes :-

(1) S3 Standard-General Purposes :-

* 99.99% Availability.
* Used for frequently accessed data.
* Low latency and high throughput.
* sustain 2 concurrent facility failures
Use-Cases :- Big data analytics,mobile & gaming application etc.

(2) S3 Standard-Infrequent Access(IA) :-

* 99.9% Availability.
* Suitable for data that is less frequently accessed but required rapid access when needed.
* It has lower cost than S3 Standard-General purpose because it access data very less frequently. 
* It take retrival fee when you access data.
* Sustain 2 concurrent facility failures.
Use-Cases :- As a data store for disaster recovery , backups etc.

(3) S3 Intelligent-Tiering :- 

* 99.9% Availability.
* Same low latency and high throughput performance of s3 Standard.
* It is designed for data with changing or unpredictable access patterns, where some objects may be accessed frequently while others are accessed infrequently.
* Cost-Optimized by automatically moving objects between two access tiers based on changing access pattern. (a) Frequent access tier :- if data is frequently accessed it automatically goes to frequent access tier . (b) InFrequent access tier :- if data is less frequently using then it automatically goes to Infrequent access tier

(4) S3 One Zone-Infrequent Access(IA) :-

* 99.5% Availability .
* Same as Infrequent-access(IA) but data is stored in a single availability zone.
* Low latency and high throughput performance.
* Lower cost compared to S3-Infrequent access(IA) by 20%.
Use-Cases :- Storing secondary backup copies of on-premise data, or storing data you can recreate.

(5) Amazon(S3) Glacier & Glacier Deep Archive :-

* 99.99%
* Amazon Glacier is used for backup and archive.
* Low cost object/file (in GB/month) meant for archiving/backup.
* The data put on glacier you expect it to be frozen.
* Data is retained for the longer term(years).
* various retrieval options of time + fees for retrieval.
* Amazon glacier is very cheap.
* There are 3 ways to retrieve data:- 
(a) expedited(1 to 5 minutes) (means data can be retrieve after request of 1 to 5 minutes) 
(b) standard (3 to 5 hours) (means data retrieve after request of 3 to 5 hours)
(c) Bulk (5 to 12 hours) (means to retrieve many files at same time , it take 5 to 12 hours after request) .

Amazon(S3) Glacier Deep Archieve :- 

* 99.99%
* It is the most cheapest storage , even it is cheaper than Glacier.
* There are only two ways to retrieval data :-
(a) Standard (12 hours)
(b) Bulk (48 hours)

################ S3 Lifecycle Rule ########################

You can transition(move) object/files between all the storage classes very easily , you can transition them using transition rules/life-cycle rule.
e.g - for infrequently(less frequent) accessed object move them to standard-infrequent acccess.
e.g - after 30 days of object creation move object to glacier class/standard class/ standard-IA class/delete object etc.
All the movement in the different storage classes can be automated using a life cycle configuration.

################# S3 Object Lock & Glacier Vault Lock #######################

S3 Object Lock :-
* Adopt WORM(Write Once Read Many)Model , It means that you write the file once to your s3 buckets , and then you will block that object version to be deleted after a specific amount of time.

* It simply means you can write in s3 object only once and ready it many time , after that no one can write or modify or deleted it even the admin , it will lock for defined/specfic timeperiod.

* It is use to secure object/file.

Glacier Vault Lock :- 
* It also folow WORM(write once Read Many) Model.
* In this we create a lock policy and this lock policy prevent future edits to that file so no longer can be changed. so once you set it no one can delete that policy.
e.g - you want to upload a file on s3 or glacier for 7 years of timeperiod and you want during this timeperiod no one can delete and modify that file , in this case you use s3 object lock or glacier vault lock.

